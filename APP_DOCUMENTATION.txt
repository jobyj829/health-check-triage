================================================================================
  HEALTH CHECK — ED TRIAGE APP
  Technical Documentation & Implementation Guide
================================================================================

Last updated: February 2026 (v13)
Data sources: MIMIC-IV v3.1 + MIMIC-IV-ED v2.2 (Beth Israel Deaconess, Boston)
              CDC NHAMCS ED Public Use Files 2018-2021 (nationally representative)


TABLE OF CONTENTS
-----------------
  1.  What This App Does
  2.  How to Run It
  3.  Architecture Overview
  4.  File-by-File Guide
  5.  Data Pipeline (Phase 1)
  6.  Model Training (Phase 2)
  7.  Interview Engine (Phase 3)
  8.  Web Application (Phase 3)
  9.  Safety System
  10. Evidence & Risk Percentages
  11. How to Swap to LLM-Powered Interview (Future)
  12. Transcript Logging & Admin Panel
  13. Model Performance & Limitations
  14. Deployment Notes
  15. Changelog


================================================================================
1. WHAT THIS APP DOES
================================================================================

A patient-facing web application that asks a series of plain-language questions
about symptoms and medical history, then recommends one of five care levels:

  Level 1 (Red)    — "Go to the Emergency Department now"
  Level 2 (Orange) — "Visit an Urgent Care Center today"
  Level 3 (Yellow) — "See your Primary Care Doctor in 1-2 days"
  Level 4 (Blue)   — "See a [Specific Specialist]" (e.g., Ophthalmologist,
                      Orthopedic Surgeon, Dermatologist, ENT, etc.)
  Level 5 (Green)  — "You're likely okay — watch for changes"

For certain complaints (belly pain, headache, urinary, pulmonary,
non-specific), the app routes to PCP first (Level 3) with a
"Your Doctor May Refer You To: [Specialist]" card, rather than
sending the patient directly to a specialist.

The recommendation is backed by:
  - A machine learning model trained on ~481K real ED visits from two sources:
      * MIMIC-IV-ED v2.2 (~425K visits, Beth Israel Deaconess, Boston)
      * CDC NHAMCS 2018-2021 (~56K visits, nationally representative survey)
  - Hard-coded red-flag safety rules that override the model
  - Conservative bias rules that escalate borderline cases
  - Complaint-to-specialist mapping from Arvig et al. WestJEM 2022
    (223,612 ED visits → ICD-10 discharge diagnosis distributions)
  - Published population-level statistics for risk percentages
  - Personalized triage nurse summary and escalation guidance
  - Differential diagnosis (top 3 most relevant diagnoses per encounter)

Designed for low medical literacy patients (6th-grade reading level).
No accounts, no PHI collected. Encounter transcripts are logged to
a local SQLite database for research purposes (admin-only access).


================================================================================
2. HOW TO RUN IT
================================================================================

Prerequisites:
  - Python 3.9+
  - The MIMIC data files (already present in mimic-iv-3.1/ and mimic-iv-ed-2.2/)

First-time setup:
  cd /Users/S183950/Desktop/Mimic
  pip3 install flask scikit-learn joblib numpy pandas duckdb

To start the app:
  cd /Users/S183950/Desktop/Mimic
  python3 run_app.py

Then open in your browser:
  http://localhost:5001

To rebuild the MIMIC dataset (if MIMIC data changes):
  python3 build_triage_dataset.py

To rebuild the NHAMCS dataset (if NHAMCS data changes):
  python3 build_nhamcs_dataset.py

To retrain the model on combined dataset:
  python3 train_triage_model.py
  (Automatically uses combined_dataset.csv.gz if available,
   otherwise falls back to triage_dataset.csv.gz)

To update the live site (Render auto-deploys from GitHub):
  git add -A && git commit -m "description" && git push origin main


================================================================================
3. ARCHITECTURE OVERVIEW
================================================================================

The app has three layers:

  PATIENT (Browser)
       |
       v
  FLASK WEB APP  (app/routes.py)
       |
       +---> Interview Engine  (app/interview_engine.py)
       |         |
       |         +---> TreeInterviewEngine  [CURRENT — zero cost]
       |         |       reads from: app/config/interview_trees/*.json
       |         |
       |         +---> LLMInterviewEngine   [FUTURE — swap in later]
       |                 calls: OpenAI / Anthropic API
       |
       +---> Triage Model  (app/model.py)
       |         |
       |         +---> Red-flag safety rules  (checked FIRST)
       |         +---> Conservative bias rules (PMH + symptom count + PCP-first)
       |         +---> Random Forest classifier  (app/models/triage_xgb.joblib)
       |         +---> Logistic Regression backup  (app/models/triage_lr.joblib)
       |         +---> Specialist selector  (app/config/complaint_specialist_map.json)
       |                 maps symptoms → primary/secondary specialist + rationale
       |                 source: Arvig et al. WestJEM 2022 (223,612 ED visits)
       |
       +---> Evidence Generator  (app/evidence.py)
       |         reads from: app/config/public_reference_rates.json
       |         builds: risk percentages, reassurance, escalation, triage summary
       |         passes through: specialist info for results card
       |
       +---> Facility Finder  (results.html JavaScript)
                uses: OpenStreetMap Nominatim + Overpass API (free, no API keys)

       +---> Transcript Logger  (app/database.py)
       |         saves to: app/transcripts.db (SQLite)
       |
       +---> Admin Panel  (app/routes.py → /admin)
                password-protected transcript browser + CSV/JSON export

  DATA PIPELINE (offline, run once per source)
       |
       build_triage_dataset.py  --->  outputs/triage_app/triage_dataset.csv.gz (MIMIC)
       build_nhamcs_dataset.py  --->  outputs/triage_app/nhamcs_dataset.csv.gz (NHAMCS)
       (merged at training)     --->  outputs/triage_app/combined_dataset.csv.gz
       train_triage_model.py    --->  app/models/*.joblib + app/config/*.json


================================================================================
4. FILE-BY-FILE GUIDE
================================================================================

ROOT LEVEL:
  run_app.py                   Entry point. Starts Flask on port 5001.
  wsgi.py                      Gunicorn entry point for production (Render).
                               Simply imports app from routes and exposes it.
  build_triage_dataset.py      Phase 1a: extracts MIMIC-IV ED visits, maps symptoms
                               and medications, assigns 5-level outcome labels.
  build_nhamcs_dataset.py      Phase 1b: processes CDC NHAMCS ED public use files
                               (2018-2021). Parses fixed-width data, maps RFV codes
                               to 42 symptom categories, maps comorbidity flags to
                               21 PMH categories, assigns 5-level triage labels from
                               disposition variables. ~56K adult visits.
  train_triage_model.py        Phase 2: trains Random Forest + Logistic Regression
                               on combined MIMIC + NHAMCS dataset (~481K visits).
                               Source-stratified train/test split. Generates red-flag
                               rules, saves compressed model artifacts.
  requirements.txt             Python dependencies.
  Procfile                     Gunicorn startup command for Render deployment.
  render.yaml                  Render blueprint (auto-deploy configuration).
  APP_DOCUMENTATION.txt        This file.

APP CODE (app/):
  __init__.py                  Package marker.
  routes.py                    Flask routes: welcome, interview loop, results,
                               processing (loading screen), admin panel.
                               Handles answer processing, mental-status red flags,
                               deferred baseline red-flag checks, session state.
                               Back button support (interview history stack).
                               Admin password: hardcoded "Password" in routes.py.
  interview_engine.py          Pluggable interview engine with two implementations:
                                 - TreeInterviewEngine (active, zero cost)
                                 - LLMInterviewEngine (stub for future)
                               Defines Question dataclass with types: single_choice,
                               multi_choice, number, text, textarea, severity_slider.
  patient_state.py             Accumulates patient answers during interview.
                               Stores: name, answering_for, age, sex, zip_code,
                               symptom_text, pmh_text, selected_symptoms, pmh,
                               interview_answers/history, red_flag_triggered.
                               Parses free-text symptoms and PMH via keyword matching.
                               Converts all answers to a feature vector for the model.
                               Neck pain mapped to dedicated interview tree but aliased
                               to back_pain feature column for the model.
  model.py                     Loads trained model at import time (preloaded during
                               server startup for fast first-request inference).
                               Makes predictions, applies safety overrides and
                               conservative bias rules.
                               Selects specific specialist for Level 4 using
                               complaint-to-diagnosis mapping from Arvig et al.
                               WestJEM 2022. Implements PCP-first routing for
                               non-specific, urologic, headache, pulmonary,
                               and belly complaints (Level 4 → Level 3 + referral).
  evidence.py                  Generates evidence for results page:
                                 - Risk percentages (immediate attention,
                                   hospitalization, mortality)
                                 - Reassurance statement (personalized)
                                 - Escalation "If This Happens" statements
                                 - Triage nurse summary (demographics, symptoms, PMH)
                                 - Watch-for signs (tailored to symptoms)
                                 - Symptom-specific published statistics
                                 - Specialist referral info (passed through from model)
                                 - Home remedies for Level 5 recommendations
                                 - Differential diagnosis (top 3 most relevant)
  database.py                  SQLite-backed transcript logging for research.
                               Functions: init_db(), save_transcript(), get_transcripts()
                               (paginated), get_transcript_by_id(), export_all_csv(),
                               export_all_json(). Database file: app/transcripts.db.

TEMPLATES (app/templates/):
  base.html                    Base layout: Tailwind CSS (CDN), nav bar, footer
                               disclaimer. Defines styles for severity slider
                               buttons, animations, HTMX transitions.
  welcome.html                 Landing page with legal disclaimer + consent checkbox.
  interview.html               One-question-per-screen interview interface.
                               Renders different UI components per question type:
                                 - single_choice: tappable option cards with icons
                                 - text / number: centered input with Continue button
                                 - textarea: multi-line input for free text
                                 - severity_slider: 4 emoji buttons (Mild, Moderate,
                                   Severe, Worst ever) with color gradient bar
                               Shows progress bar ("Name · Step X of Y"),
                               context tags ("About your headache"), skip option
                               for optional questions, and back button for navigation.
  processing.html              Loading screen shown while model inference runs.
                               Animated progress steps ("Reviewing symptoms",
                               "Comparing with published data", "Preparing
                               recommendation") with 3.5s JavaScript redirect to
                               /results. Prevents 502 timeout perception on cold starts.
  results.html                 Color-coded recommendation header (personalized with
                               patient name). Sections:
                                 - Red flag alert (if triggered)
                                 - Reassurance statement
                                 - Risk percentage bars (3 metrics: immediate
                                   attention, hospitalization, death)
                                 - Differential diagnosis (top 3 diagnoses)
                                 - "Why We're Recommending This" risk factors
                                 - "If This Happens, Get Help Right Away" escalation
                                 - "Show This to the Triage Nurse" summary card
                                 - "Try This at Home" remedies (Level 5 only)
                                 - "Nearby Facilities" finder (JS-powered, if zip given)
                                 - "What the Data Shows" published statistics
                                 - "Keep an Eye On" watch-for signs
                                 - Disclaimer
                                 - Print / Start Over buttons
  admin_login.html             Admin panel login with password input and eye toggle
                               icon for show/hide password visibility.
  admin_list.html              Paginated transcript list: timestamp, patient name,
                               age, sex, symptoms, recommendation level. Export
                               buttons for CSV and JSON. Click any row for detail.
  admin_detail.html            Full transcript view: demographics, interview Q&A,
                               prediction, risk percentages, specialist info,
                               differential diagnosis (top 3 with likelihood badges),
                               escalation statements, triage nurse summary.
                               Back button to list.

STATIC ASSETS (app/static/):
  css/style.css                Custom styles (minimal — Tailwind handles most).
  js/interview.js              Minimal JS for checkbox interactions.

CONFIG (app/config/):
  symptom_groups.json          12 plain-language symptom groups shown to the
                               patient as tappable cards. Each group maps to
                               multiple underlying symptom category IDs.
                               E.g., "Chest pain or pressure" -> [chest_pain,
                               palpitations]. The model still uses the 42
                               individual symptom flags under the hood.
  symptom_categories.json      42 symptom categories mapped from chief complaints.
                               Used internally by the model for feature encoding.
                               NOT shown directly to patients (groups are shown).
  pmh_categories.json          21 past-medical-history categories derived from
                               medication reconciliation data.
  medication_map.json          Regex patterns mapping medication therapeutic classes
                               to PMH categories.
  red_flags.json               12 hard-coded safety override rules. Each defines
                               a combination of symptoms/PMH that always triggers
                               a Level 1 (Emergency) recommendation.
  evidence_stats.json          Pre-computed aggregate statistics for each symptom.
                               Used internally for evidence generation.
  public_reference_rates.json  Publicly available, peer-reviewed statistics for
                               admission and mortality rates by symptom. Sources:
                               CDC NHAMCS 2021, Arvig et al. WestJEM 2022, Pines
                               et al. Medical Care 2015. Used for the risk
                               percentage bars on the results page. DUA-compliant
                               (no MIMIC individual patient counts exposed).
  complaint_specialist_map.json  Complaint-to-specialist mapping derived from
                               Arvig et al. WestJEM 2022 (Table 2: Chief
                               Complaints → ICD-10 Discharge Diagnoses across
                               223,612 ED visits). Maps each symptom ID to:
                                 - primary_specialist (e.g., "Ophthalmologist")
                                 - secondary_specialist (e.g., "Neurologist")
                                 - rationale (evidence-based explanation)
                                 - diagnosis_distribution (ICD-10 categories + %)
                                 - thirty_day_mortality_pct
                               Covers 35+ complaint categories. Used by model.py
                               to populate the specialist card on the results page.

INTERVIEW TREES (app/config/interview_trees/):
  chest_pain.json              Follow-up Qs: pain quality, radiation, onset, severity
                               (severity_slider type), breathing, exertion, SOB,
                               sweating, nausea, lightheaded, cardiac history.
  abdominal_pain.json          Follow-up Qs: location, quality, associated symptoms.
  fever.json                   Follow-up Qs: temperature, duration, neck stiffness.
  shortness_of_breath.json     Follow-up Qs: onset, positional, leg swelling.
  headache.json                Follow-up Qs: thunderclap, worst-ever, stiff neck,
                               severity (severity_slider type), vision, weakness.
  back_pain.json               Follow-up Qs: location, onset, cause, severity,
                               pain quality, radiation to legs, numbness/weakness,
                               saddle numbness, bladder/bowel, fever, weight loss,
                               cancer history, IV drug use, night pain. Red-flag
                               combos for cauda equina, bilateral leg weakness,
                               spinal infection, metastatic disease, and trauma
                               with weakness.
  neck_pain.json               Dedicated neck-specific follow-up Qs: location
                               (back/side/front of neck), onset, cause (car accident,
                               fall, slept wrong), severity, radiation to arms,
                               numbness in arms/hands, headache, difficulty turning
                               head, fever. Red-flag combos for spinal cord injury,
                               meningitis, post-trauma weakness.
  dizziness.json               Follow-up Qs: room spinning vs lightheaded, stroke signs.
  nausea_vomiting.json         Follow-up Qs: blood, dehydration, keep fluids down.
  _generic.json                FALLBACK tree for any symptom without a dedicated
                               tree (e.g., eye_problem, sore_throat, ear_problem,
                               rash, swelling, etc.). Asks 6 universal questions:
                               duration, severity (severity_slider), trajectory,
                               functional impact, associated fever, tried anything.
                               Ensures every complaint gets follow-up questions.

  Each file defines:
    - "symptom_id": matches the symptom category ID
    - "questions": array of branching questions
    - Each question has: id, text, type (single_choice | severity_slider), options
    - Optional "condition" field referencing a previous answer
    - "red_flag_combinations": patterns that trigger immediate Level 1

MODEL ARTIFACTS (app/models/):
  triage_xgb.joblib            Primary model: calibrated Random Forest (100 trees,
                               max_depth=12). Trained on combined MIMIC + NHAMCS
                               dataset (~385K train, ~96K test). Compressed to ~20MB.
                               Preloaded at server startup for fast inference.
  triage_lr.joblib             Backup model: Logistic Regression.
  scaler.joblib                StandardScaler fitted on training data.
  feature_columns.json         Ordered list of 67 feature column names.
                               The model expects features in this exact order.

DATA OUTPUTS (outputs/triage_app/):
  triage_dataset.csv.gz        MIMIC dataset: ~425K rows x 90 columns.
  nhamcs_dataset.csv.gz        NHAMCS dataset: ~56K rows x 74 columns.
  combined_dataset.csv.gz      Merged MIMIC + NHAMCS: ~481K rows x 69 columns.
                               Used for model training.
  dataset_summary.txt          Level distribution, ESI breakdown, top categories.
  training_report.txt          Model performance: accuracy, F1, confusion matrices,
                               source-stratified evaluation (MIMIC vs NHAMCS).

NHAMCS DATA (nhamcs_data/ — gitignored):
  ED2018, ed2019, ed2020, ed2021   Raw fixed-width data files from CDC.
  ed18inp.txt ... ed21inp.txt      SAS input format definitions (column positions).
  ed21for.txt                      SAS value label definitions (RFV code meanings).


================================================================================
5. DATA PIPELINE (Phase 1)
================================================================================

Phase 1a — build_triage_dataset.py (MIMIC-IV):

  What it does:
    1. Loads all adult ED visits from MIMIC-IV-ED
    2. Maps 60K+ free-text chief complaints to 42 symptom categories
       using regex pattern matching (e.g., "chest pain" -> "Chest Pain",
       "n/v" -> "Nausea / Throwing Up", "s/p fall" -> "Injury / Fall")
    3. Maps medication records from medrecon to 21 PMH categories
       using therapeutic class keywords (e.g., "insulin" -> "Diabetes",
       "ACE inhibitor" -> "High Blood Pressure")
    4. Links each ED visit to admission/ICU/mortality outcomes
    5. Queries labevents to flag ED stays with advanced workup:
       troponin (itemid 51002, 52642, 51003), D-dimer (50915, 51196, 52551),
       BNP (50963), lactate (50813, 52442).
    6. Assigns one of 5 triage levels based on:
         Level 1 (ED): admitted to ICU/floor, died, ESI 1, OR discharged
           but had advanced workup (troponin/D-dimer/BNP/lactate ordered)
         Level 2 (UC): discharged, NO advanced labs, basic workup only
           (CBC/BMP/UA/X-ray level), ESI 3-5
         Level 3 (Primary Care): discharged, could wait 1-2 days
         Level 4 (Specialist): discharged with specialty-appropriate ICD Dx
         Level 5 (Reassurance): self-limiting, minimal intervention
    7. Engineers 67 patient-reportable features (symptom flags + PMH flags +
       age + sex + counts)
    8. Saves dataset + config files for the app

  Key data sources joined:
    - ed/edstays.csv.gz        (disposition, timestamps)
    - ed/triage.csv.gz         (chief complaints, vitals, ESI)
    - ed/medrecon.csv.gz       (home medications)
    - ed/diagnosis.csv.gz      (ICD codes)
    - hosp/admissions.csv.gz   (admission outcomes)
    - icu/icustays.csv.gz      (ICU flag)
    - hosp/patients.csv.gz     (age, sex)
    - hosp/labevents.csv.gz    (advanced lab flag: troponin/D-dimer/BNP/lactate)

Phase 1b — build_nhamcs_dataset.py (CDC NHAMCS):

  What it does:
    1. Loads NHAMCS ED public use files for 2018-2021 (~70K total records,
       ~56K adults age ≥ 18) from the nhamcs_data/ directory. Files are
       fixed-width ASCII text downloaded from:
       ftp.cdc.gov/pub/Health_Statistics/NCHS/Datasets/NHAMCS/
    2. Parses fixed-width columns using SAS INPUT format definitions
       (ed21inp.txt) — key variables: AGE, SEX, RFV1-RFV3 (Reason for
       Visit codes), IMMEDR (ESI/immediacy), comorbidity flags
       (DIABETES, HTN, CHF, CAD, COPD, ASTHMA, CKD, ESRD, CANCER,
       EDHIV, SUBSTAB), disposition variables (ADMITHOS, OBSHOS,
       OBSDIS, DIEDED, DOA, LEFTAMA, TRANOTH, TRANPSYC)
    3. Maps Reason for Visit (RFV) codes to the same 42 symptom categories
       used by the MIMIC pipeline. RFV codes are hierarchical 5-digit
       codes from the CDC Reason for Visit Classification (RFVC).
       Mapping covers: chest pain (10500-10503), SOB (14150-14302),
       abdominal pain (15400-15453), headache (12100-12101), back pain
       (19050-19106), neck pain (19000-19006), extremity pain (19150-19606),
       fractures (5005-5050), lacerations (5205-5325), and 20+ more
       categories. Unmapped codes fall back to "other".
    4. Maps NHAMCS comorbidity flags to PMH categories:
       DIABTYP1/2/0 → Diabetes, HTN → High Blood Pressure,
       CHF/CAD → Heart Problems, ASTHMA/COPD → Asthma/COPD,
       CANCER → Cancer, CKD/ESRD → Kidney Problems,
       EDHIV → HIV/Immunocompromised. Other PMH categories are set to 0
       (NHAMCS doesn't capture medication-based PMH like MIMIC does).
    5. Assigns 5-level triage labels using NHAMCS disposition variables:
         Level 1: DIEDED=1, DOA=1, ADMITHOS=1, OBSHOS=1, TRANOTH=1,
                  TRANPSYC=1, or IMMEDR ≤ 2 (ESI 1-2)
         Level 2: OBSDIS=1 (observed then discharged) or remaining
                  discharged patients
         Level 3: IMMEDR=3 (semi-urgent) with LOV ≥ 240 min
         Level 5: IMMEDR 4-5 (low-acuity) with LOV < 240 min,
                  or LEFTAMA=1
         No Level 4 assigned (NHAMCS lacks ICD diagnosis detail for
         specialty mapping).
    6. Saves 56K-row dataset aligned to the same feature schema as MIMIC.

  Note: NHAMCS data is freely available from the CDC with no registration.
  It is a nationally representative survey weighted to estimate US ED
  visit totals. We treat each record equally (unweighted) for training.

Merged dataset (combined_dataset.csv.gz):
  Created by train_triage_model.py at training time. Loads both
  triage_dataset.csv.gz (MIMIC) and nhamcs_dataset.csv.gz (NHAMCS),
  adds a "source" column, subsets to the 67 model feature columns
  + triage_level, and saves the combined file (~481K rows).


================================================================================
6. MODEL TRAINING (Phase 2 — train_triage_model.py)
================================================================================

Dataset:
  Combined MIMIC + NHAMCS: ~481K rows
    - MIMIC: ~425K visits (single academic center, full feature coverage)
    - NHAMCS: ~56K visits (nationally representative, limited PMH)

Features (67 total, all patient-reportable):
  - 42 binary symptom category flags
  - 21 binary PMH flags
  - age (continuous)
  - gender_male (binary)
  - n_symptoms (count of selected symptoms)
  - n_comorbidities (count of PMH conditions)

Models trained:
  - Random Forest: 100 trees, max_depth=12, min_samples_leaf=20,
    class-weighted (Level 1 weight tripled to heavily penalize missing
    emergencies). Calibrated with isotonic regression (3-fold CV).
    Compressed with zlib to ~20MB. Preloaded at server startup.
  - Logistic Regression: multinomial, class-weighted, 2000 max iterations.
    Backup model for interpretability.

Training/test split: 80/20, stratified by BOTH triage level AND data source.
  Train: ~385K (340K MIMIC + 45K NHAMCS)
  Test:  ~96K  (85K MIMIC + 11K NHAMCS)

Top feature importances (Random Forest):
  1. age                    (0.220)
  2. n_comorbidities        (0.124)
  3. sym_extremity_pain     (0.112)
  4. pmh_high_blood_pressure(0.053)
  5. sym_abdominal_pain     (0.051)

Source-stratified performance:
  MIMIC:  Accuracy=0.541, F1=0.480, Level 1 Sensitivity=0.878
  NHAMCS: Accuracy=0.328, F1=0.308, Level 1 Sensitivity=0.763
  Note: NHAMCS accuracy is lower because NHAMCS has limited PMH features
  and no Level 4 (Specialist) assignments. The safety system (red flags +
  conservative bias) supplements the model for these cases.


================================================================================
7. INTERVIEW ENGINE (Phase 3 — app/interview_engine.py)
================================================================================

The interview engine is PLUGGABLE. It exposes two methods:

  get_next_question(patient_state) -> Question or None
  check_red_flags(patient_state)   -> red-flag dict or None

Current implementation: TreeInterviewEngine
  - Reads structured JSON question trees from app/config/interview_trees/
  - Zero API cost, deterministic, works offline
  - Questions are authored at 6th-grade reading level
  - Branching logic: questions can have "condition" fields that reference
    a previous question's answer

Interview flow (one question per screen):
  1. Baseline questions (always asked in this order):
     a. "What's your first name?" (text — used for personalization)
     b. "Who is this health check for?" — TWO-STEP FLOW:
        Step 1 (single_choice — 2 options):
          - I'm filling this out for myself  → continues normally
          - I'm filling this out for someone else  → goes to Step 2
        Step 2 (single_choice — 3 options, shown only if "someone else"):
          - I'm a parent / guardian filling this out for my child
            → continues normally
          - The person I'm helping is confused / not making sense
            → IMMEDIATE redirect to Level 1 Emergency results
          - The person has a medical condition preventing them from answering
            → continues normally (the interview proceeds and the decision
              tree / model determines the recommendation)
     c. "How old are you?" (number)
     d. "What is your biological sex?" (single_choice: Male / Female)
     e. "What's bothering you today?" (textarea — free text, parsed
        into symptom flags via keyword matching)
     f. "Do you have any health conditions or take any medications?"
        (textarea — free text, parsed into PMH flags via keywords)
     g. "What's your zip code?" (text — optional, skippable; used to
        find nearby ER/UC facilities on results page)
  2. Red-flag check runs after baseline is complete (see Safety System)
  3. Symptom-specific follow-ups from matching JSON trees, CAPPED at
     6 total follow-up questions (MAX_FOLLOWUPS constant). If a
     matched symptom has no dedicated tree file, the _generic.json
     fallback tree is used (asked once, not per symptom). Question
     types include single_choice and severity_slider (emoji-based:
     Mild, Moderate, Severe, Worst ever). Follow-ups show context
     tags ("About your headache") for personalization.
  4. Red-flag check after each follow-up answer
  5. Model prediction when no more questions remain

Average interview: 8-13 questions (7 baseline + up to 6 follow-ups).

Progress bar: displays "Name · Step X of Y" at top of each screen.
Total estimate is calculated dynamically via engine.estimate_total().


================================================================================
8. WEB APPLICATION (Phase 3 — app/routes.py + templates/)
================================================================================

Routes:
  GET  /             Welcome page with disclaimer + consent checkbox
  POST /start        Accept disclaimer, initialize session, redirect to interview
  GET  /interview    Show the next question (one question per screen)
  POST /answer       Process answer, update state, check red flags, redirect
  POST /back         Go back one question (pop from history stack)
  GET  /processing   Loading screen ("Compiling Your Recommendations...")
                     with animated progress steps and 3.5s JS redirect to /results
  GET  /results      Run model, show recommendation + evidence + disclaimer.
                     Saves transcript to SQLite on first load.
  GET  /restart      Clear session, return to welcome
  GET  /admin        Admin login page (password prompt with eye toggle)
  POST /admin        Verify admin password, set session flag
  GET  /admin/transcripts        Paginated transcript list (protected)
  GET  /admin/transcripts/<id>   Transcript detail view (protected)
  GET  /admin/export/csv         Download all transcripts as CSV (protected)
  GET  /admin/export/json        Download all transcripts as JSON (protected)

Session management:
  Patient state (name, answering_for, age, sex, zip_code, symptoms, PMH,
  answers, history, red_flag_triggered) is stored in Flask's encrypted
  session cookie. Transcript is saved to SQLite when results are displayed.

UI design:
  - Tailwind CSS loaded via CDN (no build step needed)
  - One question per screen (no chat history displayed)
  - Free-text input for symptoms and PMH (no body map)
  - Emoji-based severity slider (4 levels with color gradient)
  - Color-coded icons for "answering for" options (green=self, blue=family,
    amber=confused, red=unresponsive)
  - Large tappable buttons for answers (48px minimum touch target)
  - Progress bar with patient name and step count
  - Context tags on follow-ups ("About your headache")
  - Optional questions have a "Skip this step" link (zip code)
  - Fade-in animations for smooth transitions
  - Mobile-responsive (max-width 600px centered card layout)
  - Help note at bottom: "Having trouble? Call 911 if you need help"

Results page features:
  - Personalized header: "John, here are your results"
  - Color-coded recommendation banner (red/orange/yellow/blue/green)
  - Red flag alert box (if triggered)
  - Personalized reassurance statement
  - 3 risk percentage bars with color coding:
      1. Likelihood of needing immediate medical attention
      2. Likelihood of hospitalization
      3. Likelihood of death
  - Specialist detail card (Level 4, or Level 3 with PCP-first):
      Shows primary specialist, secondary specialist (if any), evidence-
      based rationale, and visual diagnosis distribution bars (ICD-10
      categories from Arvig et al. WestJEM 2022). Heading adapts:
        - Level 4: "Recommended Specialist"
        - Level 3 + PCP-first: "Your Doctor May Refer You To"
  - "Why We're Recommending This" — personalized risk factor list
  - "If This Happens, Get Help Right Away" — symptom-specific escalation
    statements in "If X → Then Y" format, color-coded by severity
    (critical=red, urgent=orange, watch=yellow)
  - "Show This to the Triage Nurse" — teal card with structured bullet
    points (demographics, chief complaint, concerning findings, PMH,
    red flags). Includes "Copy to clipboard" button. Shown for Level ≤ 3.
  - "Nearby Emergency Departments" / "Nearby Urgent Care Centers" —
    up to 5 facilities found via OpenStreetMap APIs (free, no API keys).
    Shows name, address, and "Directions" link. Falls back to
    "Open in Google Maps" link. Shown for Level ≤ 2 when zip provided.
  - "What the Data Shows" — published symptom statistics with sources
  - "Keep an Eye On" — tailored watch-for signs
  - Disclaimer
  - Print Results / Start Over buttons


================================================================================
9. SAFETY SYSTEM
================================================================================

Five layers of safety, checked in order:

Layer 1: MENTAL STATUS RED FLAGS (app/routes.py)
  Fires IMMEDIATELY during the "answering for" question — before any
  clinical data is collected. If the user selects:
    - "confused / disoriented / not making sense" → Level 1 Emergency
      (message: stroke, severe infection, emergency; call 911)
  Note: "medical condition prevents answering" does NOT trigger
  immediate ER redirect — the interview continues normally and the
  decision tree / model determines the recommendation.

Layer 2: RED-FLAG RULES (app/config/red_flags.json)
  Hard-coded overrides that fire after ALL baseline questions are answered
  (ensuring PMH and zip code are collected for the triage nurse summary).
  If a dangerous symptom combination is detected, the interview stops
  and immediately recommends Level 1 (Emergency).

  Current rules (12 total):
    - Chest pain + shortness of breath
    - Stroke symptoms (any)
    - Cardiac arrest signs (any)
    - Severe allergic reaction (any)
    - Suicidal thoughts / self-harm (any)
    - Chest pain + diabetes + age > 40
    - Shortness of breath + heart problems history
    - Blood in stool or vomit (any)
    - Altered mental status / confusion (any)
    - Seizure (any)
    - Allergic reaction + breathing difficulty
    - Abdominal pain + on blood thinners

Layer 3: MODEL SAFETY BIAS (app/model.py)
  Two-tier threshold system tuned for the MIMIC ED-population bias
  (baseline P(L1) of ~15-25% for almost every presentation):
    - If P(Level 1) > 40% and prediction is NOT Level 1 → escalate to L1
    - If P(Level 1) > 30% and prediction is Level 2 or 3 → escalate to L1
  These thresholds are high enough to avoid sending benign cases
  (sore throat, rash, blurry vision) to the ER while still catching
  genuinely risky presentations.

Layer 4: CONSERVATIVE BIAS RULES (app/model.py)
  - High-risk PMH (Heart Problems, Diabetes, Cancer, Blood Thinners,
    HIV/Immunocompromised, Organ Transplant, Kidney Problems, Liver
    Problems) + any active symptom → at minimum Level 3 (PCP).
    Reassurance and Specialist are overridden.
  - 3+ symptoms reported → at minimum Level 2 (Urgent Care).
  - PCP-first routing: if the patient's primary symptom is in the
    PCP_FIRST_SYMPTOMS set (abdominal_pain, nausea_vomiting, diarrhea,
    constipation, urinary, headache, cough, sore_throat, other) and
    the model predicts Level 4 (Specialist), the recommendation is
    downgraded to Level 3 (Primary Care Doctor) with a specialist
    referral card indicating "Your Doctor May Refer You To: [Specialist]".

Layer 5: "IF THIS HAPPENS, ESCALATE" WARNINGS (app/evidence.py)
  Every results page, regardless of recommendation level, includes
  symptom-specific escalation statements in "If X → Then Y" format:
    - Critical (red): "If chest pain spreads to arm/jaw → Call 911"
    - Urgent (orange): "If symptoms worsen → Go to ER"
    - Watch (yellow): "If no improvement in 24-48h → See a doctor sooner"
  Plus general escalation signs (trouble breathing, confusion, etc.).

Layer 6: ESCALATION GUIDANCE IN REASSURANCE
  Every recommendation level's reassurance statement includes
  tailored language encouraging the patient to seek higher-level
  care if symptoms worsen or new concerning symptoms appear.


================================================================================
10. EVIDENCE & RISK PERCENTAGES
================================================================================

The results page displays four risk percentage bars. These are generated
by app/evidence.py using a combination of:

  1. Model probabilities (from the Random Forest classifier)
  2. Published population-level statistics (from public_reference_rates.json)

The three metrics:

  a) "Likelihood of needing immediate medical attention"
     = Blended: model P(Level 1) weighted with published admission rate
     for the patient's primary symptom.

  b) "Likelihood of hospitalization"
     = Blended: model P(Level 1) × adjustment factor, anchored to
     published hospitalization rates from CDC NHAMCS data.

  c) "Likelihood of death"
     = Published mortality rate for the primary symptom, adjusted by
     patient risk factors (age, PMH).

Sources in public_reference_rates.json:
  - CDC National Hospital Ambulatory Medical Care Survey (NHAMCS) 2021
  - Arvig et al., Western Journal of Emergency Medicine, 2022
  - Pines et al., Medical Care, 2015

Specialist mapping source (complaint_specialist_map.json):
  - Arvig et al. WestJEM 2022, Table 2: Chief Complaints, Underlying
    Diagnoses, and Mortality in 223,612 Adult Non-trauma ED Visits.
    DOI: 10.5811/westjem.2022.9.56332
  - Each complaint is mapped to a primary and secondary specialist
    based on the most common ICD-10 discharge diagnosis categories.
  - Diagnosis distribution bars are shown on the results page.

DUA compliance:
  No individual MIMIC patient counts are exposed. All percentages on
  the results page come from publicly available published literature.
  The disclaimer states: "Risk percentages are based on published
  population-level data and may not reflect your individual situation."

Triage nurse summary (evidence.triage_summary):
  Generated by _build_triage_summary() in evidence.py. Contains:
    - Patient demographics (age, sex)
    - Chief complaint (free text)
    - Concerning findings from follow-up answers (e.g., "What does the
      pain feel like? Pressure or squeezing")
    - Relevant medical history
    - Red flags triggered
  Shown as a teal card on the results page for Level ≤ 3 (ER/UC/PCP)
  with a "Copy to clipboard" button.

Reassurance statement:
  Personalized based on recommendation level, patient name, symptoms,
  and risk percentage. E.g., "John, based on what you've told us,
  your symptoms are unlikely to be life-threatening. However, seeing
  your doctor will help rule out any concerns."


================================================================================
11. HOW TO SWAP TO LLM-POWERED INTERVIEW (Future)
================================================================================

The architecture was designed so the interview engine can be swapped from
structured clinical trees (free) to an LLM (GPT-4o, Claude, etc.) with
minimal code changes. Here's exactly what to do:

STEP 1: Get an API key
  - Go to https://platform.openai.com (for GPT-4o) or
    https://console.anthropic.com (for Claude)
  - Create an account, add a payment method
  - Generate an API key

STEP 2: Install the client library
  pip3 install openai        # for GPT-4o / GPT-4o-mini
  # OR
  pip3 install anthropic     # for Claude

STEP 3: Set the API key as an environment variable
  export OPENAI_API_KEY="sk-..."
  # OR
  export ANTHROPIC_API_KEY="sk-ant-..."

STEP 4: Implement LLMInterviewEngine in app/interview_engine.py

  The stub class is already there. Replace the get_next_question()
  method with something like:

  ---------------------------------------------------------------
  import openai
  import os

  class LLMInterviewEngine(InterviewEngine):
      def __init__(self):
          self.client = openai.OpenAI(api_key=os.environ["OPENAI_API_KEY"])
          self.system_prompt = """
          You are a medical triage assistant. Ask the patient ONE focused
          follow-up question based on their symptoms to help determine
          whether they need the Emergency Department, Urgent Care,
          Primary Care, a Specialist, or reassurance.

          Rules:
          - Use 6th-grade reading level language
          - Ask ONE question at a time
          - Provide 3-5 answer options the patient can tap
          - If you detect a red-flag pattern, return {"red_flag": true}
          - Return JSON: {"text": "...", "options": [...], "done": false}
          - When you have enough info, return {"done": true}
          """

      def get_next_question(self, patient_state):
          messages = [
              {"role": "system", "content": self.system_prompt},
              {"role": "user", "content": json.dumps(patient_state.summary())}
          ]
          for item in patient_state.interview_history:
              messages.append({"role": "assistant", "content": item["question_text"]})
              messages.append({"role": "user", "content": str(item["answer"])})

          response = self.client.chat.completions.create(
              model="gpt-4o-mini",   # cheapest: ~$0.003/session
              messages=messages,
              response_format={"type": "json_object"},
          )
          data = json.loads(response.choices[0].message.content)
          if data.get("done"):
              return None
          return Question(
              id=f"llm_{len(patient_state.interview_history)}",
              text=data["text"],
              question_type="single_choice",
              options=[{"value": o, "label": o} for o in data["options"]],
          )
  ---------------------------------------------------------------

STEP 5: Change ONE line in app/routes.py

  Current:
    engine = TreeInterviewEngine()

  Change to:
    from .interview_engine import LLMInterviewEngine
    engine = LLMInterviewEngine()

  Or, for a config-driven approach:
    import os
    if os.environ.get("INTERVIEW_ENGINE") == "llm":
        from .interview_engine import LLMInterviewEngine
        engine = LLMInterviewEngine()
    else:
        engine = TreeInterviewEngine()

STEP 6: Restart the app
  python3 run_app.py

That's it. The UI, model, safety rules, and evidence system are
completely decoupled from the interview engine. Nothing else changes.

COST ESTIMATES:
  - GPT-4o:       ~$0.02 - $0.05 per patient session (10 exchanges)
  - GPT-4o-mini:  ~$0.002 - $0.005 per session (recommended for MVP)
  - Claude Sonnet: ~$0.02 - $0.04 per session
  At 10,000 patients/month with GPT-4o-mini: ~$30-50/month

HYBRID OPTION:
  You can also mix both: use the structured trees for the baseline
  questions (age, sex, symptoms, PMH) and the LLM for follow-up
  questions only. This reduces API calls while keeping the
  intelligent conversation feel.


================================================================================
12. TRANSCRIPT LOGGING & ADMIN PANEL
================================================================================

Every completed triage session is automatically saved to a local SQLite
database (app/transcripts.db) when the results page loads. This provides
a complete audit trail for research and testing.

Each transcript record contains:
  - session_id (UUID), timestamp
  - Demographics: patient_name, age, sex, zip_code, answering_for
  - Raw input: symptom_text, pmh_text
  - Parsed data: selected_symptoms (JSON), pmh (JSON)
  - Full interview Q&A: interview_history (JSON array of all questions
    asked and answers given, including severity slider values)
  - Prediction: prediction_level (1-5), prediction_label
  - Risk percentages: risk_pcts (JSON: immediate_attention,
    hospitalization, death)
  - Specialist info: specialist_info (JSON)
  - Reassurance statement, escalation statements (JSON)
  - Triage nurse summary
  - Red flag info (if triggered)
  - Risk factors (JSON array)
  - Differential diagnosis: top 3 diagnoses with likelihood and notes

Admin access:
  URL: /admin
  Password: "Password" (hardcoded in routes.py)
  Features:
    - Paginated list of all encounters (25 per page)
    - Click any row for full detail view
    - Export all transcripts as CSV or JSON
    - Password input has eye toggle for show/hide visibility

Note on Render free tier: the SQLite file lives on disk and is ephemeral —
it is reset on each deployment. For production use, a persistent database
(PostgreSQL) should be used instead.


================================================================================
13. MODEL PERFORMANCE & LIMITATIONS
================================================================================

Current model performance (Random Forest on combined test set, ~96K visits):
  Overall accuracy:    51.7%
  F1 (weighted):       45.4%
  Level 1 sensitivity: 87.1% (model alone — safety rules supplement this)

Source-stratified performance:
  MIMIC  (85K test): Accuracy=54.1%, F1=48.0%, L1 Sensitivity=87.8%
  NHAMCS (11K test): Accuracy=32.8%, F1=30.8%, L1 Sensitivity=76.3%

Why accuracy is moderate:
  The model uses ONLY patient-reportable features (symptoms + PMH + age/sex).
  It does NOT have access to vital signs, lab results, or physical exam
  findings, which are the strongest predictors of admission. This is by
  design — a patient at home doesn't have a blood pressure cuff or blood
  work. The model provides a reasonable baseline; the safety system (red-flag
  rules + conservative bias + escalation warnings) catches critical cases.

Why NHAMCS accuracy is lower:
  - NHAMCS has limited PMH features (only 7 comorbidity flags vs 21
    medication-derived flags in MIMIC)
  - NHAMCS doesn't have Level 4 (Specialist) assignments since it lacks
    detailed ICD diagnosis data
  - RFV code-to-symptom mapping is an approximation (vs MIMIC's direct
    chief complaint text)
  - Despite lower overall accuracy, Level 1 sensitivity (76.3%) is
    acceptable — the safety system catches remaining emergencies

Key limitations:
  1. MIMIC-dominant training: ~88% of training data is from a single
     academic center (Beth Israel Deaconess, Boston). The NHAMCS data
     (~12%) provides national breadth but is less feature-rich.
  2. Selection bias: we only have data on patients who came to the ER. We
     don't know about patients who stayed home successfully.
  3. Chief complaint gap: MIMIC chief complaints are nurse-written; NHAMCS
     uses coded RFV; the app collects patient-reported free text. The
     interview follow-up questions help bridge this gap.
  4. No vitals: the model doesn't use vital signs because patients can't
     reliably self-report blood pressure, oxygen saturation, etc.

Future improvements:
  - Add optional vital signs input (for patients with home monitors)
  - Add wearable device integration (Apple Watch HR, SpO2)
  - Additional external validation datasets
  - LLM-powered interview for more nuanced symptom capture
  - A/B testing different model architectures
  - Pediatric-specific question trees (for parent/child flow)
  - Use NHAMCS survey weights for nationally representative training


================================================================================
14. DEPLOYMENT NOTES
================================================================================

For local development:
  python3 run_app.py
  Open http://localhost:5001

Current production deployment:
  Platform: Render (free tier)
  GitHub repo: jobyj829/health-check-triage
  Auto-deploy: enabled (push to main triggers redeploy)
  Entry point: wsgi.py → Gunicorn (via Procfile)
  Startup command: gunicorn wsgi:app --bind 0.0.0.0:$PORT --workers 2 --timeout 120

  To update the live site:
    git add -A && git commit -m "description" && git push origin main
    (Render auto-deploys within 2-5 minutes)

  Key Render configuration:
    - Procfile: web: gunicorn wsgi:app --bind 0.0.0.0:$PORT --workers 2 --timeout 120
    - render.yaml: blueprint with build/start commands, env vars
    - Model preloading: the 20MB ML model loads at Gunicorn worker startup,
      not on first request. This makes deployments ~10s longer but eliminates
      cold-start latency for users.
    - Loading screen: /processing route shows animated "Compiling Your
      Recommendations" for 3.5 seconds before redirecting to /results,
      preventing perceived delays during model inference.

  Render free tier limitations:
    - RAM: 512MB (sufficient for 20MB model + Flask)
    - Disk: ephemeral (SQLite transcripts.db reset on each deploy)
    - Cold starts: ~15-30s after 15 min of inactivity
    - For production: upgrade to paid tier or use PostgreSQL for transcripts

For alternative deployment:
  1. Change SECRET_KEY in routes.py (or set SECRET_KEY env variable)
  2. Use a production WSGI server:
       pip3 install gunicorn
       gunicorn -w 4 -b 0.0.0.0:5001 wsgi:app
  3. Put behind HTTPS (nginx reverse proxy + Let's Encrypt)
  4. Add rate limiting (flask-limiter)
  5. The app is stateless (session is in the cookie) — it scales
     horizontally with no shared state

DUA compliance:
  The app ships trained model weights and aggregate statistics only.
  No individual MIMIC patient records are ever exposed to the end user.
  Risk percentages on the results page are sourced from publicly
  available published literature (CDC NHAMCS, WestJEM, Medical Care).
  Model coefficients and tree structures are derived statistics, not
  "data" per the PhysioNet Credentialed Health Data License.
  NHAMCS data is public domain (CDC public use files, no DUA required).

External API dependencies (results page, optional):
  - OpenStreetMap Nominatim: geocodes zip code to lat/lng (free, no key)
  - OpenStreetMap Overpass API: finds nearby hospitals/clinics (free, no key)
  - Both are called client-side via JavaScript; app works without them
  - Falls back to Google Maps search link if APIs are unavailable


================================================================================
15. CHANGELOG
================================================================================

v1 — Initial release
  - Data pipeline (build_triage_dataset.py)
  - Model training (train_triage_model.py)
  - Flask web app with card-based symptom selection
  - Chat-style interview interface
  - 12 red-flag safety rules
  - Results page with evidence statistics

v2 — Simplification
  - Removed medical jargon from symptom prompts
  - Redefined Urgent Care criteria (discharge only, no advanced imaging)
  - Simplified symptom selection to 12 plain-language groups

v3 — Free text + risk percentages
  - Replaced symptom cards with free-text input for initial symptoms
  - Replaced PMH checkboxes with free-text input
  - Added risk percentage bars (immediate attention, hospitalization, death)
  - DUA compliance: removed all patient count mentions from UI
  - Sourced percentages from publicly available literature

v4 — UX overhaul
  - One question per screen (replaced chat history)
  - Interactive SVG body map for symptom location
  - Emoji-based severity sliders (Mild / Moderate / Severe / Worst ever)
  - Patient name for personalization
  - Mental status immediate flag (confused / unresponsive)
  - "If This Happens, Escalate" statements
  - Personalized reassurance statement
  - "Likelihood of something serious" percentage
  - Context tags on follow-up questions ("About your headache")
  - Conservative body map symptom mappings (removed high-severity auto-triggers)

v5 — Conservative bias + triage tools
  - Refined "Who is this for?" to 5 specific options:
    myself / parent for child / family helping / confused / can't respond
  - Conservative bias: ER threshold lowered 15% → 10%, PMH-based
    escalation, 3+ symptoms → at minimum UC
  - Zip code question (optional, skippable) for facility lookup
  - "Nearby Emergency Departments" / "Nearby Urgent Care Centers" via
    OpenStreetMap APIs (free, no API keys)
  - "Show This to the Triage Nurse" summary card with copy-to-clipboard
  - "When in doubt, seek care" message for Level 3+ recommendations
  - Red flags deferred until baseline complete (PMH + zip collected first)
  - Mental status flags still fire immediately

v6 — Streamlined interview
  - Removed body map — symptoms captured via free text only
  - Capped follow-up questions at 6 max (MAX_FOLLOWUPS = 6) to keep
    the interview fast and focused
  - Fixed facility finder: Overpass API query now uses nwr (node/way/
    relation) instead of node-only, with 32km search radius, timeout,
    and deduplication. Verified returning real hospitals for 10001 (NYC)
    and 90210 (Beverly Hills)
  - Cleaned up unused body map CSS and JavaScript
  - Total interview: 7 baseline + up to 6 follow-ups = 13 questions max

v7 — Two-step answering-for + home remedies
  - "Answering for" split into two questions:
    Step 1: "For yourself or someone else?" (2 options)
    Step 2 (only if "someone else"): "Why?" with 3 specific reasons:
      a. Parent/guardian of a child → continues normally
      b. Confused and unable to answer → IMMEDIATE ER redirect
      c. Medical condition prevents them from answering → IMMEDIATE ER redirect
  - "Try This at Home" section for Level 5 (reassurance) recommendations:
    green card with numbered, symptom-specific home remedies (rest, hydrate,
    OTC medication, cold compress, etc.). Covers 13+ symptom categories
    with specific guidance per symptom. Falls back to general remedies
    if no symptom-specific match.
  - Safety bias retuned: ER escalation threshold now uses a two-tier
    system — 25% for Level 5 predictions (high-confidence benign), 15%
    for Level 2-4. This allows truly benign cases (mild headache in a
    healthy 25-year-old) to correctly get "You're Likely Okay" while
    still catching all genuinely risky cases.

v8 — Chronic condition fix
  - Corrected "medical condition prevents answering" option: no longer
    triggers IMMEDIATE ER redirect. Instead, the interview continues
    normally and the decision tree / model determines the recommendation.
    Only "confused / not making sense" still triggers immediate ER.

v9 — Specialist mapping + PCP-first routing + symptom parsing fixes (current)
  - Complaint-to-specialist mapping:
    Added complaint_specialist_map.json derived from Arvig et al.
    WestJEM 2022 (Table 2: 223,612 ED visits → ICD-10 discharge
    diagnosis distributions). Covers 35+ complaint categories.
    Level 4 recommendations now name a SPECIFIC specialist:
      e.g., "See an Ophthalmologist", "See an Orthopedic Surgeon",
      "See a Dermatologist", "See an ENT Specialist", etc.
    Results page shows a specialist detail card with:
      - Primary specialist name
      - Secondary specialist (if applicable)
      - Evidence-based rationale from the paper
      - Visual diagnosis distribution bars (ICD-10 categories)
    Grammar handled dynamically (a/an based on specialist name).
  - PCP-first routing for select complaints:
    For non-specific, urologic, headache, pulmonary, and belly complaints
    (PCP_FIRST_SYMPTOMS in model.py), Level 4 predictions are downgraded
    to Level 3 (Primary Care Doctor) with a "Your Doctor May Refer You To"
    card showing the specialist as a PCP referral.
    Rationale: these complaints benefit from PCP evaluation and possible
    referral rather than direct specialist consultation.
    Affected symptoms: abdominal_pain, nausea_vomiting, diarrhea,
    constipation, urinary, headache, cough, sore_throat, other.
  - Personalized reassurance statements:
    Level 3 + PCP-first: mentions the specific specialist as a referral.
    Level 4 (direct specialist): names the specialist and secondary.
  - Generic fallback interview tree (_generic.json):
    Symptoms without a dedicated tree (eye_problem, sore_throat, rash,
    swelling, etc.) now get 6 universal follow-up questions via a
    generic tree: duration, severity (slider), trajectory, functional
    impact, fever, tried anything. Prevents immediate model prediction
    without follow-up for uncommon complaints.
  - Symptom parsing fixes:
    "neck" patterns added to back_pain keywords (previously missed "my
    neck is hurting"). back_pain.json updated with "Neck" as a location
    option and label changed to "Back or Neck Pain".
    eye_problem regex fixed: \beye\b → \beyes?\b (now matches "eyes").
  - Safety bias thresholds raised:
    P(L1) > 40% for any non-L1 → escalate to L1
    P(L1) > 30% for Level 2-3 → escalate to L1
    Previous thresholds (15%/25%) were too low for the MIMIC ED-population
    bias (baseline P(L1) of ~15-25% even for benign cases). New thresholds
    prevent false ER escalation for sore throat, rash, blurry vision,
    mild headache, etc.
  - Public reference rates updated:
    Mortality rates in public_reference_rates.json updated with 0-7 day
    and 8-30 day mortality from Arvig et al. WestJEM 2022 for chest_pain,
    shortness_of_breath, abdominal_pain, fever, headache, weakness.


v10 — Transcript logging + admin panel + back button
  - Transcript logging:
    Added app/database.py with SQLite backend. Every completed encounter
    is saved to app/transcripts.db with full demographics, interview Q&A,
    prediction, risk percentages, specialist info, escalation statements,
    and triage nurse summary. Session UUID prevents duplicates on refresh.
  - Admin panel:
    Password-protected admin interface at /admin. Features:
      * Paginated transcript list (25 per page)
      * Detailed transcript view with all interview data
      * CSV and JSON export of all transcripts
      * Eye icon toggle for password visibility
    Default password: "Password" (hardcoded in routes.py).
  - Back button:
    Users can navigate backwards during the interview. Interview history
    is maintained as a stack; pressing back pops the last answer and
    re-asks the previous question.
  - Differential diagnosis:
    Added differential diagnosis to evidence.py. For each encounter,
    generates a list of possible diagnoses based on symptoms, sorted by
    acuity (serious conditions prioritized for ER/UC recommendations)
    and likelihood. Capped at top 3 most relevant diagnoses.
    Stored in transcript and displayed on admin detail page.

v11 — Deployment to GitHub + Render
  - Deployed to jobyj829/health-check-triage GitHub repository
  - Set up Render web service with auto-deploy from main branch
  - Created wsgi.py entry point for Gunicorn
  - Added Procfile and render.yaml blueprint
  - Retrained model with smaller footprint (100 trees, max_depth=12)
    to reduce file size from 672MB to 32MB for GitHub compatibility

v12 — Neck pain fix + loading screen + model preload
  - Neck pain interview:
    Separated "neck pain" from "back pain" with its own dedicated
    interview tree (neck_pain.json) with neck-specific follow-up
    questions: location (back/side/front), cause (car accident, fall,
    slept wrong), radiation to arms, numbness in hands, headache,
    difficulty turning head, fever. Red flags for spinal cord injury,
    meningitis, and post-trauma weakness. Neck pain aliased to
    back_pain feature column for model compatibility.
  - Loading screen:
    Added /processing route with animated "Compiling Your
    Recommendations" page. Shows three progress steps that animate
    in sequence (reviewing symptoms, comparing with data, preparing
    recommendation). JavaScript redirects to /results after 3.5 seconds.
    Prevents perceived delays and 502 timeouts on cold starts.
  - Model preloading:
    ML model now loads at module import time (during Gunicorn worker
    startup) rather than on first request. Eliminates 8-15 second
    first-request delay at the cost of slightly longer deployments.
  - Admin password fix:
    Hardcoded ADMIN_PASSWORD = "Password" in routes.py to bypass
    Render environment variable override issues.

v13 — NHAMCS multi-center training data (current)
  - Integrated CDC NHAMCS ED public use files (2018-2021):
    Downloaded 4 years of nationally representative ED visit data from
    ftp.cdc.gov (~70K total records, ~56K adult visits).
  - Created build_nhamcs_dataset.py:
    Parses fixed-width CDC data files using column specs derived from
    SAS INPUT statements. Maps NHAMCS Reason for Visit (RFV) codes to
    the app's 42 symptom categories. Maps comorbidity flags (DIABETES,
    HTN, CHF, CAD, COPD, ASTHMA, CKD, ESRD, CANCER, EDHIV) to 7 of
    the 21 PMH categories. Assigns 5-level triage labels from disposition
    variables (ADMITHOS, DIEDED, DOA, IMMEDR, LOV).
  - Merged datasets:
    Combined MIMIC (~425K visits) + NHAMCS (~56K visits) = ~481K total
    visits. Added "source" column for tracking.
  - Retrained model on combined dataset:
    Source-stratified 80/20 train/test split. 100 trees, max_depth=12.
    Overall Level 1 sensitivity: 87.1%. Compressed model: 20MB.
    MIMIC subset: Acc=54.1%, NHAMCS subset: Acc=32.8%.
  - Updated train_triage_model.py:
    Automatically loads combined_dataset.csv.gz when available.
    Source-stratified evaluation reports per-source metrics.
  - The model is now trained on multi-center, nationally representative
    data rather than single-center MIMIC data only.


================================================================================
  END OF DOCUMENTATION
================================================================================
